{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from scipy.io import savemat, loadmat\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "\n",
    "\n",
    "def data_extract():\n",
    "    df = pd.read_csv(\"Cleaned_Project_Data.csv\", low_memory=False, usecols=[0, 1, 4, 6])\n",
    "    print(df[df.isnull().T.any()])\n",
    "    print(df.shape)\n",
    "    print(df.columns)\n",
    "    df[\"Award Date\"] = pd.to_datetime(df[\"Award Date\"])\n",
    "    # for i in df.index:\n",
    "    #     if int(df[\"Award Date\"].dt.year[i]) < 2012:\n",
    "    #         df.drop(i, inplace=True)\n",
    "    df.drop(index=df[df[\"Award Date\"].dt.year < 2016].index, inplace=True)\n",
    "    print(df.shape)\n",
    "    # df[\"New_Description\"] = df[df.columns[:2]].apply(lambda x: \". \".join(x.dropna()), axis=1)\n",
    "    df = df.fillna(\"a\")\n",
    "    # print(df[df.isnull().T.any()])\n",
    "    # df2 = pd.DataFrame({\"Description\": df[\"Description\"]})\n",
    "    df[\"New_Description\"] = df[\"Title\"] + df[\"Description\"]\n",
    "    df[\"New_Description\"] = (\n",
    "        df[\"New_Description\"]\n",
    "            .apply(lambda x: str(x))\n",
    "            .str.split()\n",
    "            .apply(lambda x: np.unique(x))\n",
    "            .str.join(' ')\n",
    "    )\n",
    "    df.to_csv(\"description.csv\", index=False, encoding=\"utf_8_sig\")\n",
    "\n",
    "\n",
    "# data_extract()\n",
    "#\n",
    "df1 = pd.read_csv(\"description.csv\")\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append(\"project\")\n",
    "stopwords.append(\"community\")\n",
    "stopwords.append(\"groups\")\n",
    "stopwords.append(\"funding\")\n",
    "stopwords.append(\"use\")\n",
    "stopwords.append(\"grants\")\n",
    "stopwords.append(\"grant\")\n",
    "stopwords.append(\"costs\")\n",
    "stopwords.append(\"programme\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stopwords[:5])\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "#\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14638357, 1)\n",
      "Index(['words'], dtype='object')\n",
      "(517139, 200000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Environment\\Python3.8\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", 'abov', 'ani', 'becaus', 'befor', 'communiti', 'cost', 'could', 'doe', 'dure', 'fund', 'group', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'programm', 'sha', 'themselv', 'veri', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# totalvocab_stemmed = []\n",
    "# totalvocab_tokenized = []\n",
    "# for i in df1[\"New_Description\"]:\n",
    "#     allwords_stemmed = tokenize_and_stem(i)  # for each item in 'synopses', tokenize/stem\n",
    "#     totalvocab_stemmed.extend(allwords_stemmed)  # extend the 'totalvocab_stemmed' list\n",
    "#     allwords_tokenized = tokenize_only(i)\n",
    "#     totalvocab_tokenized.extend(allwords_tokenized)\n",
    "# vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index=totalvocab_stemmed)\n",
    "# print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "# vocab_frame.to_csv(\"vocab_frame.csv\", index=True)\n",
    "#\n",
    "vocab_frame = pd.read_csv(\"vocab_frame.csv\", index_col=0)\n",
    "print(vocab_frame.shape)\n",
    "print(vocab_frame.columns)\n",
    "totalvocab_stemmed = vocab_frame.index.tolist()\n",
    "totalvocab_tokenized = vocab_frame[\"words\"].tolist()\n",
    "#\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.7, max_features=200000,\n",
    "                                   stop_words=stopwords,\n",
    "                                   use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1, 3))\n",
    "#\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df1[\"New_Description\"])\n",
    "tfidf_standardized = StandardScaler(with_mean=False).fit(tfidf_matrix)\n",
    "\n",
    "save_dict = {'name': 'matrix', 'data': tfidf_standardized}\n",
    "savemat('test.mat', save_dict)\n",
    "\n",
    "\n",
    "# tfidf_standardized = loadmat('test.mat')\n",
    "print(tfidf_matrix.shape)\n",
    "#\n",
    "#\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 16\u001B[0m\n\u001B[0;32m      1\u001B[0m num_clusters \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# km = KMeans(n_clusters=num_clusters)\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# km.fit(tfidf_matrix)\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# clusters = km.labels_.tolist()\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# joblib.dump(lda, 'lda.pkl')\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# joblib.dump(lda, 'lda2.pkl')\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m lda \u001B[38;5;241m=\u001B[39m \u001B[43mjoblib\u001B[49m\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlda.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# #\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# # km = joblib.load('doc_cluster.pkl')\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# # clusters = km.labels_.tolist()\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m#         print('%s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=', ')\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m#     print()  # add whitespace\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprint_top_words\u001B[39m(model, feature_names, n_top_words):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "num_clusters = 50\n",
    "# km = KMeans(n_clusters=num_clusters)\n",
    "# km.fit(tfidf_matrix)\n",
    "# clusters = km.labels_.tolist()\n",
    "# joblib.dump(km, 'doc_cluster.pkl')\n",
    "#\n",
    "# lda = LatentDirichletAllocation(\n",
    "#     n_components=num_clusters,\n",
    "#     max_iter=50,\n",
    "#     learning_method='online',\n",
    "#     learning_offset=50,\n",
    "#     random_state=0)\n",
    "# lda.fit(tfidf_matrix)\n",
    "# joblib.dump(lda, 'lda.pkl')\n",
    "# joblib.dump(lda, 'lda2.pkl')\n",
    "lda = joblib.load('lda.pkl')\n",
    "# #\n",
    "# # km = joblib.load('doc_cluster.pkl')\n",
    "# # clusters = km.labels_.tolist()\n",
    "#\n",
    "#\n",
    "# df1[\"cluster\"] = clusters\n",
    "# print(df1[\"cluster\"].value_counts())\n",
    "#\n",
    "# print(\"Top terms per cluster:\")\n",
    "# print()\n",
    "# # sort cluster centers by proximity to centroid\n",
    "# order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "# for i in range(num_clusters):\n",
    "#     print(\"Cluster %d words: \" % i, end='')  # %d功能是转成有符号十进制数 #end=''让打印不要换行\n",
    "#     for ind in order_centroids[i, :6]:  # replace 6 with n words per cluster\n",
    "#         print('%s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=', ')\n",
    "#     print()  # add whitespace\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "      for topic_idx, topic in enumerate(model.components_):\n",
    "          print(\"Topic #%d:\" % topic_idx)\n",
    "          for i in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "              print('%s' % vocab_frame.loc[terms[i].split(' ')].values.tolist()[0][0],\n",
    "                    end=\" \")\n",
    "          print()\n",
    "\n",
    "\n",
    "# pyLDAvis.enable_notebook()\n",
    "# pic = pyLDAvis.sklearn.prepare(lda, tfidf_matrix, tfidf_vectorizer)\n",
    "# pyLDAvis.save_html(pic, 'lda_pass' + str(50) + '.html')\n",
    "# pyLDAvis.show(pic)\n",
    "\n",
    "\n",
    "print_top_words(lda, terms, 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-87602984",
   "language": "python",
   "display_name": "PyCharm (SCC.460 Coursework)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}