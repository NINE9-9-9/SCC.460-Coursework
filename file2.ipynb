{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from scipy.io import savemat, loadmat\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "\n",
    "\n",
    "def data_extract():\n",
    "    df = pd.read_csv(\"Cleaned_Project_Data.csv\", low_memory=False, usecols=[0, 1, 4, 6])\n",
    "    print(df[df.isnull().T.any()])\n",
    "    print(df.shape)\n",
    "    print(df.columns)\n",
    "    df[\"Award Date\"] = pd.to_datetime(df[\"Award Date\"])\n",
    "    # for i in df.index:\n",
    "    #     if int(df[\"Award Date\"].dt.year[i]) < 2012:\n",
    "    #         df.drop(i, inplace=True)\n",
    "    df.drop(index=df[df[\"Award Date\"].dt.year < 2016].index, inplace=True)\n",
    "    print(df.shape)\n",
    "    # df[\"New_Description\"] = df[df.columns[:2]].apply(lambda x: \". \".join(x.dropna()), axis=1)\n",
    "    df = df.fillna(\"a\")\n",
    "    # print(df[df.isnull().T.any()])\n",
    "    # df2 = pd.DataFrame({\"Description\": df[\"Description\"]})\n",
    "    df[\"New_Description\"] = df[\"Title\"] + df[\"Description\"]\n",
    "    df[\"New_Description\"] = (\n",
    "        df[\"New_Description\"]\n",
    "            .apply(lambda x: str(x))\n",
    "            .str.split()\n",
    "            .apply(lambda x: np.unique(x))\n",
    "            .str.join(' ')\n",
    "    )\n",
    "    df.to_csv(\"description.csv\", index=False, encoding=\"utf_8_sig\")\n",
    "\n",
    "\n",
    "# data_extract()\n",
    "#\n",
    "df1 = pd.read_csv(\"description.csv\")\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append(\"project\")\n",
    "stopwords.append(\"community\")\n",
    "stopwords.append(\"groups\")\n",
    "stopwords.append(\"funding\")\n",
    "stopwords.append(\"use\")\n",
    "stopwords.append(\"grants\")\n",
    "stopwords.append(\"grant\")\n",
    "stopwords.append(\"costs\")\n",
    "stopwords.append(\"programme\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stopwords[:5])\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "#\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# totalvocab_stemmed = []\n",
    "# totalvocab_tokenized = []\n",
    "# for i in df1[\"New_Description\"]:\n",
    "#     allwords_stemmed = tokenize_and_stem(i)  # for each item in 'synopses', tokenize/stem\n",
    "#     totalvocab_stemmed.extend(allwords_stemmed)  # extend the 'totalvocab_stemmed' list\n",
    "#     allwords_tokenized = tokenize_only(i)\n",
    "#     totalvocab_tokenized.extend(allwords_tokenized)\n",
    "# vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index=totalvocab_stemmed)\n",
    "# print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "# vocab_frame.to_csv(\"vocab_frame.csv\", index=True)\n",
    "#\n",
    "vocab_frame = pd.read_csv(\"vocab_frame.csv\", index_col=0)\n",
    "print(vocab_frame.shape)\n",
    "print(vocab_frame.columns)\n",
    "totalvocab_stemmed = vocab_frame.index.tolist()\n",
    "totalvocab_tokenized = vocab_frame[\"words\"].tolist()\n",
    "#\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.7, max_features=200000,\n",
    "                                   stop_words=stopwords,\n",
    "                                   use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1, 3))\n",
    "#\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df1[\"New_Description\"])\n",
    "tfidf_standardized = StandardScaler(with_mean=False).fit(tfidf_matrix)\n",
    "\n",
    "save_dict = {'name': 'matrix', 'data': tfidf_standardized}\n",
    "savemat('test.mat', save_dict)\n",
    "\n",
    "\n",
    "# tfidf_standardized = loadmat('test.mat')\n",
    "print(tfidf_matrix.shape)\n",
    "#\n",
    "#\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "num_clusters = 50\n",
    "# km = KMeans(n_clusters=num_clusters)\n",
    "# km.fit(tfidf_matrix)\n",
    "# clusters = km.labels_.tolist()\n",
    "# joblib.dump(km, 'doc_cluster.pkl')\n",
    "#\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=num_clusters,\n",
    "    max_iter=50,\n",
    "    learning_method='online',\n",
    "    learning_offset=50,\n",
    "    random_state=0)\n",
    "lda.fit(tfidf_matrix)\n",
    "joblib.dump(lda, 'lda.pkl')\n",
    "\n",
    "\n",
    "# #\n",
    "# # km = joblib.load('doc_cluster.pkl')\n",
    "# # clusters = km.labels_.tolist()\n",
    "#\n",
    "#\n",
    "# df1[\"cluster\"] = clusters\n",
    "# print(df1[\"cluster\"].value_counts())\n",
    "#\n",
    "# print(\"Top terms per cluster:\")\n",
    "# print()\n",
    "# # sort cluster centers by proximity to centroid\n",
    "# order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "# for i in range(num_clusters):\n",
    "#     print(\"Cluster %d words: \" % i, end='')  # %d功能是转成有符号十进制数 #end=''让打印不要换行\n",
    "#     for ind in order_centroids[i, :6]:  # replace 6 with n words per cluster\n",
    "#         print('%s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=', ')\n",
    "#     print()  # add whitespace\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "print_top_words(lda, terms, 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}